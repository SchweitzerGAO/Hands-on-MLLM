# Transformer model related
vocab_size = 50
hidden_size = 512
max_len = 4096
num_layers = 3
n_heads = 8

# Transformer training related
seed = 42
max_context_len = 16
batch_size = 4
lr = 4e-5
epoch = 3000
log_step = 100
